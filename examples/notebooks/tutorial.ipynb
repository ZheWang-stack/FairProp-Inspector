{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# FairProp Inspector - Interactive Tutorial\n",
                "\n",
                "Welcome to the FairProp Inspector tutorial! This notebook will guide you through:\n",
                "\n",
                "1. **Installation & Setup**\n",
                "2. **Basic Usage**\n",
                "3. **Training a Custom Model**\n",
                "4. **Batch Processing**\n",
                "5. **Performance Analysis**\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Installation & Setup\n",
                "\n",
                "First, let's install FairProp Inspector:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install from source\n",
                "!pip install -e ..\n",
                "\n",
                "# Or install from GitHub\n",
                "# !pip install git+https://github.com/ZheWang-stack/FairProp-Inspector.git"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import required libraries\n",
                "import sys\n",
                "import os\n",
                "\n",
                "# Add parent directory to path\n",
                "sys.path.insert(0, os.path.abspath('..'))\n",
                "\n",
                "from src.inference.predict import predict\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Set style\n",
                "sns.set_style('whitegrid')\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "\n",
                "print(\"‚úì Setup complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Basic Usage\n",
                "\n",
                "Let's start with a simple example:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example 1: Clear violation\n",
                "text1 = \"No kids under 12 allowed\"\n",
                "label1, confidence1 = predict(text1, \"../artifacts/model\")\n",
                "\n",
                "print(f\"Text: '{text1}'\")\n",
                "print(f\"Label: {label1}\")\n",
                "print(f\"Confidence: {confidence1:.1%}\")\n",
                "print()\n",
                "\n",
                "# Example 2: Compliant text\n",
                "text2 = \"Great school district nearby\"\n",
                "label2, confidence2 = predict(text2, \"../artifacts/model\")\n",
                "\n",
                "print(f\"Text: '{text2}'\")\n",
                "print(f\"Label: {label2}\")\n",
                "print(f\"Confidence: {confidence2:.1%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Interactive Testing\n",
                "\n",
                "Try your own examples:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Try your own text here!\n",
                "your_text = \"Perfect for young professionals\"  # Change this\n",
                "\n",
                "label, confidence = predict(your_text, \"../artifacts/model\")\n",
                "\n",
                "print(f\"Text: '{your_text}'\")\n",
                "print(f\"Label: {label}\")\n",
                "print(f\"Confidence: {confidence:.1%}\")\n",
                "\n",
                "if label == \"NON_COMPLIANT\":\n",
                "    print(\"\\n‚ö†Ô∏è WARNING: This text may violate Fair Housing Act guidelines!\")\n",
                "else:\n",
                "    print(\"\\n‚úì This text appears compliant.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Batch Processing\n",
                "\n",
                "Process multiple property descriptions at once:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sample property descriptions\n",
                "properties = [\n",
                "    \"Beautiful 3BR home in quiet neighborhood\",\n",
                "    \"No kids under 12 allowed\",\n",
                "    \"Perfect for young professionals\",\n",
                "    \"Great school district nearby\",\n",
                "    \"Christian community preferred\",\n",
                "    \"Wheelchair accessible entrance\",\n",
                "    \"No section 8\",\n",
                "    \"Walking distance to shops\",\n",
                "    \"Ideal for active adults\",\n",
                "    \"Family-friendly neighborhood\"\n",
                "]\n",
                "\n",
                "# Process all\n",
                "results = []\n",
                "for text in properties:\n",
                "    label, confidence = predict(text, \"../artifacts/model\")\n",
                "    results.append({\n",
                "        'text': text,\n",
                "        'label': label,\n",
                "        'confidence': confidence\n",
                "    })\n",
                "\n",
                "# Create DataFrame\n",
                "df = pd.DataFrame(results)\n",
                "df"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Analyze Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Summary statistics\n",
                "print(\"Summary:\")\n",
                "print(f\"Total: {len(df)}\")\n",
                "print(f\"Violations: {(df['label'] == 'NON_COMPLIANT').sum()}\")\n",
                "print(f\"Compliant: {(df['label'] == 'COMPLIANT').sum()}\")\n",
                "print(f\"Violation Rate: {(df['label'] == 'NON_COMPLIANT').mean():.1%}\")\n",
                "print(f\"Average Confidence: {df['confidence'].mean():.1%}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize results\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Plot 1: Label distribution\n",
                "df['label'].value_counts().plot(kind='bar', ax=axes[0], color=['#00ff41', '#ff4444'])\n",
                "axes[0].set_title('Label Distribution', fontsize=14, fontweight='bold')\n",
                "axes[0].set_xlabel('Label')\n",
                "axes[0].set_ylabel('Count')\n",
                "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=0)\n",
                "\n",
                "# Plot 2: Confidence distribution\n",
                "df.boxplot(column='confidence', by='label', ax=axes[1])\n",
                "axes[1].set_title('Confidence by Label', fontsize=14, fontweight='bold')\n",
                "axes[1].set_xlabel('Label')\n",
                "axes[1].set_ylabel('Confidence')\n",
                "plt.suptitle('')  # Remove default title\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Performance Analysis\n",
                "\n",
                "Let's measure inference latency:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import time\n",
                "\n",
                "# Measure latency\n",
                "latencies = []\n",
                "test_text = \"No kids under 12 allowed\"\n",
                "\n",
                "# Warmup\n",
                "for _ in range(5):\n",
                "    predict(test_text, \"../artifacts/model\")\n",
                "\n",
                "# Actual measurement\n",
                "for _ in range(100):\n",
                "    start = time.time()\n",
                "    predict(test_text, \"../artifacts/model\")\n",
                "    latency = (time.time() - start) * 1000  # ms\n",
                "    latencies.append(latency)\n",
                "\n",
                "# Statistics\n",
                "import numpy as np\n",
                "print(f\"Mean latency: {np.mean(latencies):.2f}ms\")\n",
                "print(f\"Median latency: {np.median(latencies):.2f}ms\")\n",
                "print(f\"P95 latency: {np.percentile(latencies, 95):.2f}ms\")\n",
                "print(f\"P99 latency: {np.percentile(latencies, 99):.2f}ms\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize latency distribution\n",
                "plt.figure(figsize=(12, 5))\n",
                "\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.hist(latencies, bins=30, color='#00ff41', alpha=0.7, edgecolor='black')\n",
                "plt.axvline(np.mean(latencies), color='red', linestyle='--', label=f'Mean: {np.mean(latencies):.1f}ms')\n",
                "plt.axvline(np.percentile(latencies, 95), color='orange', linestyle='--', label=f'P95: {np.percentile(latencies, 95):.1f}ms')\n",
                "plt.xlabel('Latency (ms)')\n",
                "plt.ylabel('Frequency')\n",
                "plt.title('Latency Distribution', fontweight='bold')\n",
                "plt.legend()\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.boxplot(latencies, vert=True)\n",
                "plt.ylabel('Latency (ms)')\n",
                "plt.title('Latency Box Plot', fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Advanced: Custom Training\n",
                "\n",
                "For training a custom model, see the [Training Guide](../docs/training_guide.md).\n",
                "\n",
                "Quick example:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example training data format\n",
                "training_data = [\n",
                "    {\"text\": \"No kids under 12 allowed\", \"label\": \"NON_COMPLIANT\"},\n",
                "    {\"text\": \"Great school district nearby\", \"label\": \"COMPLIANT\"},\n",
                "    # ... more examples\n",
                "]\n",
                "\n",
                "print(\"Training data format:\")\n",
                "print(training_data[0])\n",
                "print(\"\\nTo train:\")\n",
                "print(\"python src/trainer/train.py --data your_data.json --epochs 3\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üéì Next Steps\n",
                "\n",
                "1. **Explore Examples**: Check out `examples/` directory for more code samples\n",
                "2. **Read Training Guide**: Learn how to train custom models in `docs/training_guide.md`\n",
                "3. **Run Benchmarks**: Test performance with `benchmarks/accuracy_comparison.py`\n",
                "4. **Contribute**: See `CONTRIBUTING.md` for contribution guidelines\n",
                "\n",
                "---\n",
                "\n",
                "## üìö Resources\n",
                "\n",
                "- [GitHub Repository](https://github.com/ZheWang-stack/FairProp-Inspector)\n",
                "- [Documentation](https://github.com/ZheWang-stack/FairProp-Inspector/blob/main/README.md)\n",
                "- [Fair Housing Act Guidelines](https://www.hud.gov/program_offices/fair_housing_equal_opp)\n",
                "\n",
                "---\n",
                "\n",
                "**Questions?** Open an issue on GitHub or email: fairprop-inspector@proton.me"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}